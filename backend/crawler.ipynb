{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import os\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.set_lang(\"simple\")\n",
    "\n",
    "all_links = set(os.listdir(\"./articles_v2\"))\n",
    "path = \"./articles_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_crawler(start_topic, depth=5):\n",
    "    visited_topics = set()\n",
    "    topics_to_visit = [(start_topic, depth)]\n",
    "\n",
    "    while topics_to_visit:\n",
    "        current_topic, current_depth = topics_to_visit.pop(0)\n",
    "        try:\n",
    "            if current_depth <= 0:\n",
    "                continue\n",
    "            \n",
    "            if current_topic in visited_topics:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                page = wikipedia.page(current_topic)\n",
    "            except wikipedia.exceptions.DisambiguationError as e:\n",
    "                page = wikipedia.page(e.options[0])\n",
    "            except wikipedia.exceptions.PageError:\n",
    "                continue\n",
    "\n",
    "            if page.title in visited_topics:\n",
    "                continue\n",
    "            \n",
    "            filepath = os.path.join(path, page.title+\".txt\")\n",
    "            file = open(filepath,'w',encoding='utf-8')\n",
    "            file.write(page.content)\n",
    "            file.close()\n",
    "\n",
    "            \n",
    "            visited_topics.add(current_topic)\n",
    "            \n",
    "            links = page.links\n",
    "            for link in links:\n",
    "                if link not in all_links:\n",
    "                    topics_to_visit.append((link, current_depth - 1))\n",
    "        except Exception as e:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crawling(topics_and_depths):\n",
    "    threads = []\n",
    "    for topic, depth in topics_and_depths:\n",
    "        thread = threading.Thread(target=wikipedia_crawler, args=(topic, depth))\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\searchex\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\mateu\\anaconda3\\envs\\searchex\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "topics_and_depths = [('Modern_era', 2), ('Middle_Ages', 2), ('Ancient_history', 2), ('Prehistory', 2), ('Adolf_Hitler', 2), ('Poland', 3), ('Timelines_of_modern_history', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('List_of_best-selling_video_games', 3), ('List_of_highest-grossing_films', 3), ('List_of_most-watched_television_broadcasts', 3), ('Pirates_of_the_Caribbean_(film_series)', 2), ('Riot_Games', 2), ('List_of_prominent_operas', 3), ('Ekstraklasa', 3)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('Africa', 2), ('Asia', 2), ('North_America', 2), ('South_America', 2), ('Antarctica', 2), ('Europe', 2), ('Oceania', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('Africa', 2), ('Asia', 2), ('North_America', 2), ('South_America', 2), ('Antarctica', 2), ('Europe', 2), ('Oceania', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\searchex\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\mateu\\anaconda3\\envs\\searchex\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "topics_and_depths = [('Harry_Potter', 3), ('Hobbit', 3), ('The_Lord_of_the_Rings', 3), ('Board_game', 3), ('Literature', 3), ('Film', 3)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\anaconda3\\envs\\searchex\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\mateu\\anaconda3\\envs\\searchex\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "topics_and_depths = [('Eurogame', 2), ('Chess', 2), ('Music_genre', 2), ('Culture', 2), ('The_Witcher', 2), ('League_of_Legends', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('Lionel_Messi', 2), ('Sport', 2), ('Cristiano_Ronaldo', 2), ('Wilfredo_León', 2), ('Robert_Lewandowski', 2), ('Jan-Krzysztof_Duda', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('Christianity', 2), ('Islam', 2), ('Hinduism', 2), ('Buddhism', 2), ('Folk_religion', 2), ('Atheism', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('Film_genre', 2), ('Literary_genre', 2), ('Agatha_Christie', 2), ('Olga_Tokarczuk', 2), ('Bolesław_Prus', 2), ('George_Orwell', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_and_depths = [('Africa', 2), ('Asia', 2), ('North_America', 2), ('South_America', 2), ('Antarctica', 2), ('Europe', 2), ('Oceania', 2)]\n",
    "run_crawling(topics_and_depths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searchex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
